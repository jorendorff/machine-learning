# To read (papers)

Top 5:

-   Vaswani et al. Attention is all you need. 2017.
    https://arxiv.org/pdf/1706.03762v5.pdf

-   Elhage et al. A mathematical framework for transformer circuits. Dec 2021.
    https://transformer-circuits.pub/2021/framework/index.html

-   Tenney et al. What do you learn from context? 2019.
    https://arxiv.org/pdf/1905.06316.pdf

-   Graves, Alex. Generating sequences with recurrent neural networks. 2014.
    https://arxiv.org/pdf/1308.0850.pdf

-   Kitaev and Klein. Constituency parsing with a self-attentive encoder. July 2018.


## More on parsing

What is a "discriminative" model? What is a "chart-based" model?

-   James Henderson. Inducing History Representations for Broad Coverage
    Statistical Parsing. 2003.
    https://aclanthology.org/N03-1014.pdf

-   Greg Durrett and Dan Klein. Neural crf parsing. July 2015.

-   Wei et al. In-order chart-based dependency parsing. 2021.
    https://arxiv.org/pdf/2102.04065.pdf

-   Stern et al. A minimal span-based neural constituency parser.
    https://aclanthology.org/P17-1076.pdf

    "An alternative line of work focuses on *chart parsers*, which use
    log-linear or neural scoring potentials to parameterize a tree-structured
    dynamic program for maximization or marginalization."

-   Hall et al. Less grammar, more features.

I have found it harder to get into this literature. It seems to require some
actual knowledge. ... It makes me want to try just using transformers and
transfer learning and see what happens.


## Honorable mention

-   Kiros et al. Skip-thought vectors. 2015.
    https://arxiv.org/pdf/1506.06726.pdf

-   (the DistBelief paper) Dean et al. Large scale distributed deep networks. 2012.
    https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf

    About parallelization of training across machines.

-   Mikolov et al. Advances in pre-training distributed word representations. 2017.
    https://arxiv.org/pdf/1712.09405.pdf

    4-page follow-up to the word2vec paper. I think not super important for me.

-   (the BERT paper) Devlin et al. BERT: Pre-training of deep bidirectional transformers for language understanding. 2018.
    https://arxiv.org/pdf/1810.04805.pdf

-   Poli et al. Hyena hierarchy: towards larger convolutional language models.
    Stanford research blog post, March 2023.

    https://hazyresearch.stanford.edu/blog/2023-03-07-hyena

    Can't quite read this yet.

-   Stern, Andreas, and Klein. A Minimal Span-Based Neural Constituency Parser.
    https://aclanthology.org/P17-1076.pdf

-   Gu, Goel, R. Efficiently Modeling Long Sequences with Structured State Spaces. Oct 2021.

-   2017 AlphaZero https://blog.acolyer.org/2018/01/10/mastering-chess-and-shogi-by-self-play-with-a-general-reinforcement-learning-algorithm/

-   2017 AlphaGo Zero https://blog.acolyer.org/2017/11/17/mastering-the-game-of-go-without-human-knowledge/

-   (the ALiBi paper) Train Short, Test Long: Attention with linear biases
    enables input length extrapolation. https://arxiv.org/pdf/2108.12409.pdf

-   (the GPT paper) Radford et al. Improving language understanding by generative pre-training. 2018.
    https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

-   Nanda et al. Progress measures for grokking via mechanistic interpretability. Jan 2023.
    https://arxiv.org/pdf/2301.05217.pdf

-   HOGWILD! https://arxiv.org/pdf/1106.5730.pdf

-   Bahdanau et al. Neural machine translation by jointly learning to align and
    translate. 2014. https://arxiv.org/abs/1409.0473

-   Cho et al. Learning phrase representations using RNN encoder-decoder for
    statistical machine translation. 2014. https://arxiv.org/abs/1406.1078

-   Sutskever et al. Sequence to sequence learning with neural networks. 2014.
    https://arxiv.org/abs/1409.3215

# Videos and courses

## CS224N: Natural Language Processing with Deep Learning

http://web.stanford.edu/class/cs224n/

Videos of the 2021 version of the course are on youtube.
https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ

This is the course that has its own podcast. Very much want to do this one.

## fastbook

fast.ai has a book, covering PyTorch:
https://github.com/fastai/fastbook/blob/master/01_intro.ipynb

and a course: https://course.fast.ai/

I wonder if I can just read all those notebooks. There's also a MOOC and an
O'Reilly book; it's bizarrely confusing.


## Computational linear algebra

https://github.com/fastai/numerical-linear-algebra/blob/master/README.md

Ten videos, top-down.
